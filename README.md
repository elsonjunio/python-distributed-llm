# python-distributed-llm

Python Distributed LLM
ğŸ“Œ VisÃ£o Geral

Este projeto Ã© um estudo sobre execuÃ§Ã£o distribuÃ­da de modelos de linguagem usando Python e PyTorch, com foco em rodar modelos grandes em mÃ¡quinas com pouca memÃ³ria RAM.

A ideia principal Ã© dividir as camadas do modelo em mÃºltiplos workers, que podem estar em processos ou mÃ¡quinas diferentes, e processar sequencialmente as partes do modelo via rede (TCP/IP).

Foram realizados testes bem-sucedidos com os modelos:

   - microsoft/Phi-3-mini-instruct

   - microsoft/Phi-3.5-mini-instruct

---

ğŸ— Arquitetura

O projeto segue um modelo Manager â†’ Workers:

#### Manager (Master)

- Recebe o *prompt* de entrada.

- Tokeniza usando `AutoTokenizer`.

- Orquestra a execuÃ§Ã£o chamando cada worker na sequÃªncia correta.

- NÃ£o carrega nenhuma camada do modelo na memÃ³ria (somente embeddings, config e tokenizer).

#### Workers

- Cada worker carrega **apenas um subconjunto das camadas** do modelo.

- Executam o *forward* local das camadas que possuem.

- Retornam os *hidden states* processados para o prÃ³ximo worker via rede.

ğŸ“¡ ComunicaÃ§Ã£o

   - A comunicaÃ§Ã£o Ã© feita via **sockets TCP/IP**.

   - SerializaÃ§Ã£o usando `pickle`.

   - O protocolo inclui prefixo de tamanho (4 bytes) para controle do fluxo de dados.

---

ğŸ¯ MotivaÃ§Ã£o

Modelos de linguagem grandes normalmente precisam de **vÃ¡rios GB de RAM** para rodar.
Este projeto demonstra que Ã© possÃ­vel distribuir a execuÃ§Ã£o em mÃºltiplos nÃ³s modestos (como **Orange Pi 4GB**), processando partes do modelo de forma sequencial.

Isso permite:

   - Usar hardware barato para estudar modelos grandes.

   - Executar localmente modelos que nÃ£o caberiam em um Ãºnico dispositivo.


ğŸ“‚ Estrutura do Projeto

```bash
src/
â”œâ”€â”€ gpt2/                     # VersÃ£o experimental para GPT-2
â”‚   â”œâ”€â”€ master/                # Scripts do manager
â”‚   â”œâ”€â”€ worker1/               # Scripts do primeiro worker
â”‚   â””â”€â”€ worker2/               # Scripts do segundo worker
â”œâ”€â”€ phi3/                      # VersÃ£o para modelos Phi-3 / Phi-3.5
â”‚   â”œâ”€â”€ manager.py             # Manager (master)
â”‚   â”œâ”€â”€ partial_forward_client.py
â”‚   â”œâ”€â”€ partial_forward_server.py
â”‚   â”œâ”€â”€ phi3_partial/          # ImplementaÃ§Ã£o de forward parcial no modelo
â”‚   â”œâ”€â”€ worker_1.py            # Primeiro worker
â”‚   â”œâ”€â”€ worker_2.py            # Segundo worker
â”‚   â””â”€â”€ test/                  # Testes e exemplos
```

ğŸš€ Como Executar

1ï¸âƒ£ **Instalar dependÃªncias**
```bash
poetry install
```

2ï¸âƒ£ **Iniciar os Workers**

Em terminais separados, rode:
```bash
poetry run python src/phi3/worker_1.py
poetry run python src/phi3/worker_2.py
```

3ï¸âƒ£ **Iniciar o Manager**
```bash
poetry run python src/phi3/manager.py
```

ğŸ”§ **ConfiguraÃ§Ã£o**

Modelo: definido em `MODEL_PATH` no cÃ³digo.

Workers: definidos na lista `WORKERS` no `manager.py`.

Portas: ajustadas diretamente nos scripts dos workers.

DivisÃ£o de camadas: controlada via parÃ¢metros `handle_section_index` e `total_sections` no carregamento `from_pretrained_partial`.

ğŸ“œ **LicenÃ§a**

Este projeto Ã© apenas para estudo e uso pessoal.
LicenÃ§a: MIT
